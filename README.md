# Gradient Boosting Model Hyperparameter Tuning

Welcome to the Gradient Boosting Model Hyperparameter Tuning project! In this repository, we have conducted an in-depth analysis of a Gradient Boosting Classifier with various hyperparameters using the scikit-learn library. Our goal is to optimize the model's performance by tuning hyperparameters and evaluating its impact on classification tasks.

## Project Overview

The project includes the following key components:

1. Data Loading and Preprocessing: We load the Wine dataset and preprocess it for model training and evaluation.

2. Hyperparameter Tuning: We explore a range of hyperparameters, including learning rates, max depths, number of estimators, loss functions, subsamples, and minimum samples for splits and leaves.

3. Model Training and Evaluation: We train Gradient Boosting Classifier models with different hyperparameter combinations and evaluate their performance.

4. Logging and Visualization: We log hyperparameters and performance metrics to [Wandb.ai](https://wandb.ai/) to visualize and compare the results.

## Viewing Experiment Reports

To view detailed experiment reports, including hyperparameters, performance metrics, and visualizations, please visit our project on [Wandb.ai](https://wandb.ai/imjoseba/vinit-upgrade/reports/GradientBoosting-WineClassification-Wandb--Vmlldzo1ODk3MDUy?accessToken=e582hu9n7dc58qmfocj90uwf5crtxv3nd8u0ck1s10ccit21efmj18lk8c021t16).

## Getting Started

If you'd like to replicate or explore the experiments locally, follow these steps:

1. Clone the repository to your local machine.

2. Set up a Python environment with the necessary dependencies, including scikit-learn and Wandb.


3. Run the provided Jupyter Notebook to perform hyperparameter tuning and experiment tracking with Wandb.

4. Analyze the experiment reports and model performance.

## Feedback and Contributions

We value your feedback and contributions. If you have any questions, suggestions, or would like to collaborate on this project, please feel free to reach out.

Happy hyperparameter tuning!

- Joseba Moreno
Data Analyst
